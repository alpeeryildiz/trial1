{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Activation, Lambda, Dense,BatchNormalization, Dropout, LSTM,  Conv2D, Conv1D, Flatten, MaxPooling2D, AveragePooling2D, Input, ZeroPadding2D, Reshape , Add\n",
    "from keras.models import Model\n",
    "import keras.backend as backend\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from scipy import misc\n",
    "from sklearn import preprocessing\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import optimizers\n",
    "import pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import RecurrencePlot\n",
    "import pickle\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, base,Trials\n",
    "import csv\n",
    "dfz = pd.read_excel('newdata.xlsx')\n",
    "df=dfz[:5665]\n",
    "df=df.drop('Date (GMT)',axis=1)\n",
    "df=df.fillna(method='ffill')\n",
    "setz=[0,1,2,3,4,5,7,9,11,12,13,15,17,25,26,27,28,30,32,48,49,50,\n",
    "      51,52,53,54,55,56,59,60,61,62,63,65,67,70,72,74,77,80,83,85,88,91,166,167,168,170,172] \n",
    "df=df.iloc[:,setz]\n",
    "df=df.dropna(axis=1,how='all')\n",
    "\n",
    "\n",
    "def Preprocessin(df2, setz ,windowlength=30, prediction_length=5, perz=0.2, threshz='distance'):\n",
    "    dftrain=df.copy()[300:4000]\n",
    "    dfvalid=df.copy()[4000:5220]\n",
    "    dftest = df.copy()[5220:]\n",
    "\n",
    "    sclr = preprocessing.StandardScaler()\n",
    "    sclr = sclr.fit(dftrain)\n",
    "\n",
    "    trainscaled = sclr.transform(dftrain)\n",
    "    valscaled = sclr.transform(dfvalid)\n",
    "    testscaled = sclr.transform(dftest)\n",
    "\n",
    "    trainarray=trainscaled[:,3].copy()                                   #extract array of particular feature  \n",
    "    validarray=valscaled[:,3].copy()                                    \n",
    "    testarray=testscaled[:,3].copy()              #,tmean,tvar=tanz(arrayzz[:,3].copy(), True)   \n",
    "    print(trainarray.shape)         \n",
    "\n",
    "    pagestrain = trainarray.shape[0] - windowlength - prediction_length\n",
    "    pagesvalid = validarray.shape[0]  - windowlength - prediction_length\n",
    "    pagestest = testarray.shape[0] - windowlength - prediction_length-2\n",
    "\n",
    "    OUTPUT_train = np.asarray([[ trainarray[i+k+windowlength] for i in range(prediction_length)]  for k in range( pagestrain)])\n",
    "    OUTPUT_val = np.asarray([[validarray[i+k+windowlength] for i in range(prediction_length)] for k in range(pagesvalid)])\n",
    "    OUTPUT_test = np.asarray([[ testarray[i+k+windowlength] for i in range(prediction_length )] for k in range(pagestest)])\n",
    "\n",
    "    for feature in range(np.array(dftrain).shape[1]):\n",
    "        print(feature)\n",
    "        trainarray=trainscaled[:,feature].copy()                                   #extract array of particular feature  \n",
    "        validarray=valscaled[:,feature].copy()                                    \n",
    "        testarray=testscaled[:,feature].copy()                 \n",
    "        #save 2d time series data as 3d list with batch_size=windowlength\n",
    "        trainarray = np.array([[[ trainarray[i+k] for i in  range(windowlength)] for t in range(1)] for k in range(pagestrain)])\n",
    "        validarray = np.array([[[validarray[i+k] for i in range(windowlength)] for t in range(1)] for k in range(pagesvalid)]) \n",
    "        testarray = np.array([[[ testarray[i+k] for i in range(windowlength)]for t in range(1)] for k in range(pagestest)])\n",
    "        if feature == 0:\n",
    "            INPUT_train_lstm = trainarray.copy()\n",
    "            INPUT_valid_lstm = validarray.copy()\n",
    "            INPUT_test_lstm = testarray.copy()\n",
    "        else:\n",
    "            INPUT_train_lstm = np.append(INPUT_train_lstm,trainarray,axis=1)\n",
    "            INPUT_valid_lstm = np.append(INPUT_valid_lstm,validarray,axis=1)\n",
    "            INPUT_test_lstm=np.append(INPUT_test_lstm,testarray,axis=1)\n",
    "\n",
    "    INPUT_train_lstm=INPUT_train_lstm.swapaxes(1,2)\n",
    "    INPUT_valid_lstm=INPUT_valid_lstm.swapaxes(1,2)\n",
    "\n",
    "    return  INPUT_train_lstm, INPUT_test_lstm,INPUT_valid_lstm,OUTPUT_val, np.array(OUTPUT_train), np.array(OUTPUT_test) #, tmean, tvar\n",
    "\n",
    "\n",
    "input_train,input_test,input_valid,out_valid,out_train,out_test = Preprocessin(df,setz)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((input_train,out_train))\n",
    "train_data = train_data.cache().shuffle(10000).batch(56).repeat()\n",
    "val_data = tf.data.Dataset.from_tensor_slices((input_valid,out_valid))\n",
    "val_data = val_data.shuffle(10000).batch(56).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelcall(dictz): \n",
    "    f_c1=int(dictz['f_c1'])\n",
    "    f_c2=int(dictz['f_c2'])\n",
    "    f_ls=int(dictz['f_ls'])\n",
    "    c1kernel=int(dictz['c1kernel'])\n",
    "    c2kernel=int(dictz['c2kernel'])\n",
    "    c1stride=int(dictz['c1stride'])\n",
    "    epochz=int(dictz['epochz'])\n",
    "    learningrate = int(dictz['learningrate'])\n",
    "    pooling = int(dictz['pooling'])  \n",
    "    dropz = int(dictz['dropz'])  \n",
    "    inp = tf.keras.layers.Input(shape=(30,49))\n",
    "    x = tf.keras.layers.Conv1D(f_c1,kernel_size=3,activation='relu')(inp)\n",
    "    x = tf.keras.layers.Conv1D(f_c2,kernel_size=2,activation='relu')(x)\n",
    "    if pooling:\n",
    "        x = tf.keAddras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.LSTM(f_ls,return_sequences=True,activation='relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(dropz)(x)\n",
    "    out = tf.keras.layers.Dense(5)(x)\n",
    "    model=tf.keras.Model(inp,out)\n",
    "    stepsperepoch=np.floor(input_train.shape[0]/56)\n",
    "    stepsperval=np.floor(input_valid.shape[0]/56)\n",
    "    rms = tf.keras.optimizers.RMSprop(learning_rate=learningrate,decay=0.01,momentum=0.85)\n",
    "    model=tf.keras.Model(inp,out)\n",
    "    model.compile(optimizer=rms,loss='mean_squared_error')\n",
    "    hist=model.fit(train_data,validation_data=val_data,shuffle=False,epochs=epochz,steps_per_epoch=stepsperepoch,validation_steps=stepsperval)\n",
    "    return {\n",
    "        'loss': min(hist.history['loss']),\n",
    "        'val_loss': min(hist.history['val_loss']),\n",
    "        'status': STATUS_OK,\n",
    "          }\n",
    "  \n",
    "spacez=hp.choice('a',[\n",
    "      {  \n",
    "    'f_c1' : hp.uniform('f_c1', 64,128 ),\n",
    "    'f_c2' : hp.uniform('f_c2', 128,256 ),\n",
    "    'f_ls' : hp.uniform('f_ls', 32,128 ),      \n",
    "    'c1kernel' : hp.choice('c1kernel', [2,3,4] ),\n",
    "    'c2kernel' : hp.choice('c2kernel', [2,3,4] ),\n",
    "    'c1stride' : hp.choice('c1stride', [1,2] ),\n",
    "    'epochz' : hp.uniform('epochz', 100,3000),\n",
    "    'learningrate' : hp.uniform('learningrate', 0.005,0.1),\n",
    "    'pooling' : hp.choice('pooling', [True,False] ),\n",
    "    'dropz' : hp.uniform('dropz', 0,0.4 )\n",
    "      }])\n",
    "trials=Trials()\n",
    "best = fmin(modelcall,\n",
    "            space=spacez,\n",
    "            algo=tpe.suggest,\n",
    "            trials=trials,\n",
    "            max_evals=25)\n",
    "w = csv.writer(open(\"tuning111111111.csv\", \"w\"))\n",
    "for key, val in best.items():\n",
    "    w.writerow([key, val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
